#!/usr/bin/env python
# coding=utf-8

from flask import jsonify, request
from rest_api_server import app
import os
from util import version, logger
import json
from auxiliar.newdatadomain import DataDomain

_log = logger.get_logger(__name__)

# Globals - Move to ctxdoing class/object
asup_file_save_path = None
asup_auto_cores_location = None
asup_elysium_serial_number = None
selected_replication_contexts = None

ASUP_FILE_INPUT_METHODS = {
    'FILE_UPLOAD': 1,
    'AUTO_CORES_PATH': 2,
    'ELYSIUM_SERIAL_NUMBER': 3
    }
asup_file_input_method = None

@app.route("/api/asup/file", methods=['POST'])
def asup_file_upload():
    global asup_file_save_path, asup_file_input_method

    _log.info("ASUP file uploaded: %s", request.files)

    f = request.files['asup']
    asup_file_save_path = os.path.join(app.config['RUNTIME_WORKING_DIR'], f.filename)
    f.save(asup_file_save_path)
    asup_file_input_method = ASUP_FILE_INPUT_METHODS['FILE_UPLOAD']
    _log.info('[asup_file_input_method=FILE_UPLOAD] ASUP file saved locally as: %s', asup_file_save_path)

    return (jsonify({}),
            200,
            {'ContentType': 'application/json'})

@app.route("/api/asup/auto_cores_path", methods=['POST'])
def asup_file_auto_cores_path():
    global asup_auto_cores_location, asup_file_input_method

    data = json.loads(request.data)
    asup_auto_cores_location = data['auto_cores_path']
    asup_file_input_method = ASUP_FILE_INPUT_METHODS['AUTO_CORES_PATH']
    _log.info('[asup_file_input_method=AUTO_CORES_PATH] ASUP file located at: %s', asup_auto_cores_location)

    return (jsonify({}),
            200,
            {'ContentType': 'application/json'})

@app.route("/api/asup/elysium_serial_number", methods=['POST'])
def asup_file_elysium_serial_number():
    global asup_elysium_serial_number, asup_file_input_method

    data = json.loads(request.data)
    asup_elysium_serial_number = data['elysium_serial_number']
    asup_file_input_method = ASUP_FILE_INPUT_METHODS['ELYSIUM_SERIAL_NUMBER']
    _log.info('[asup_file_input_method=ELYSIUM_SERIAL_NUMBER] Serial Number: %s', asup_elysium_serial_number)

    return (jsonify({}),
            200,
            {'ContentType': 'application/json'})

@app.route("/api/asup/analysis/replication_contexts", methods=['GET', 'POST'])
def replication_contexts_list():
    global selected_replication_contexts, asup_file_input_method,dd # I do convert in global de dd object of class DataDomain

    #My code comes after here
    _log.info(asup_file_input_method)
    if(asup_file_input_method==1): #File has been uploaded

        _log.info(asup_file_save_path)
        asup_file_save_path_escaped=asup_file_save_path.encode("utf-8")
        _log.info(asup_file_save_path_escaped)
        dd=DataDomain(asup_file_save_path_escaped)
    if request.method == 'GET':
        # Call DataDomain to obtain a DataDomain object obtained from the autosupports
        # and analyze the ASUP file and get a list of repl ctx
        # Create an object, TODO if-else for different asup_file_input_method
        # ctxdoing = AsupAnalyzer(asup_file_input_method, asup_file_save_path)
        # repl_ctx_list = ctxdoing.get_repl_ctx_list()

        _log.info(dd.replication_contexts)

        repl_ctx_list = []
        for index,item in enumerate(dd.replication_contexts_frontend):
            _log.info("Comprobando")
            if(dd.is_source_context(index)):
                _log.info(item)
                repl_ctx_list.append(item)


        _log.info("Found %d replication contexts", len(repl_ctx_list))

        return (jsonify(repl_ctx_list),
                200,
                {'ContentType': 'application/json'})

    elif request.method == 'POST':
        selected_replication_contexts = json.loads(request.data)


        return (jsonify({}),
                200,
                {'ContentType': 'application/json'})

@app.route("/api/asup/analysis/replication_contexts/time_spent", methods=['GET'])
def analyze_replication_contexts():
    # Call ctxdoing to analyze selected replication contexts
    # 'data' is a list similar to that returned by ctxdoing.get_repl_ctx_list()
    # result = ctxdoing.analyze_repl_ctx(data)
    resultado=[]
    print("Selected Replication Contexts:")
    print(selected_replication_contexts)
    print("estad√≠sticas")
    print(dd.lrepl_client_time_stats)

    # We iterate between the selected replication contexts

    for itera_dict in selected_replication_contexts: # selected_replication_contexts is the info that comes from the selection context screen
        print("iterando")
        print("itera dict:{}".format(itera_dict))

        print(itera_dict['ctx'])
        dic_auxiliar={}
        dic_auxiliar['ctxDetails']=itera_dict
        list_ctx_usage_time=[]

        #resultado.append(dic_auxiliar)
        dic_auxiliar_2={'graphhImage': 'assets/replicationgraph.png'}
        dic_auxiliar.update(dic_auxiliar_2)
        #resultado.append(dic_auxiliar)
        aux_lrepl_client_time_stats=[] # This list is going to content the lrepl client time stats of that particular context
        for i in range(1,len(dd.lrepl_client_time_stats)): # We start in 1, because dd,lrepl_client_time_stats[0], just contains the header
            print("Lo obtenido:{}".format(dd.lrepl_client_time_stats[i]))
            searching_for="rctx://"+str(itera_dict['ctx'])
            print("We are searching for:{}".format(searching_for))
            if(dd.lrepl_client_time_stats[i][0]==searching_for): # We are searching for on one of the specific contexts selected for analysis
                print("he encontrado lo que andaba buscando")
                # We found it, so we make the aux_lrepl_client_time_stats equal to the list that corresponds in the dd object
                aux_lrepl_client_time_stats=dd.lrepl_client_time_stats[i]
                # And we exit
                break

        print("aux_lrepl_client_time_stats:{}".format(aux_lrepl_client_time_stats))
        # We do the maths
        sum=0
        for x in range(1,len(aux_lrepl_client_time_stats)):
            print("sum:{}".format(sum))
            sum=sum+int(aux_lrepl_client_time_stats[x])

        print("la lista con los tiempos del contexto:{}, es: {}".format(itera_dict['ctx'],aux_lrepl_client_time_stats))
        total_computed_time=sum
        print("total computed time:{}".format(total_computed_time))
        # We are using always relative times due to the inacurracy of send_file to compute the total amount of a context, and being even 0 most of the times
        dic_ctx_usage_time={}
        dic_ctx_usage_time={'key':'Total time spent by the replication context','value':total_computed_time,'unit': 'seconds'}
        list_ctx_usage_time.append(dic_ctx_usage_time)
        dic_ctx_usage_time={}

        # Time sending references

        tsr_percentage=float(aux_lrepl_client_time_stats[2])*100/total_computed_time
        dic_ctx_usage_time={'key': 'Time sending references', 'value':tsr_percentage,'unit': '%' }
        list_ctx_usage_time.append(dic_ctx_usage_time)
        dic_ctx_usage_time={}

        # Time sending segments

        tss_percentage=float(aux_lrepl_client_time_stats[3])*100/total_computed_time
        dic_ctx_usage_time={'key': 'Time sending segments', 'value':tss_percentage, 'unit': '%' }
        list_ctx_usage_time.append(dic_ctx_usage_time)
        dic_ctx_usage_time={}

        # Time receiving references
        trr_percentage=float(aux_lrepl_client_time_stats[4])*100/total_computed_time
        dic_ctx_usage_time={ "key": "Time receiving references", "value": trr_percentage, "unit": "%" }
        list_ctx_usage_time.append(dic_ctx_usage_time)
        dic_ctx_usage_time={}

        # Time waiting for references from destination
        twrd_percentage=float(aux_lrepl_client_time_stats[5])*100/total_computed_time
        dic_ctx_usage_time={ "key": "Time waiting for references from destination", "value": twrd_percentage, "unit": "%" }
        list_ctx_usage_time.append(dic_ctx_usage_time)
        dic_ctx_usage_time={}

        # Time waiting getting references
        twgr_percentage=float(aux_lrepl_client_time_stats[6])*100/total_computed_time
        dic_ctx_usage_time={ "key": "Time waiting getting references", "value": twgr_percentage, "unit": "%" }
        list_ctx_usage_time.append(dic_ctx_usage_time)
        dic_ctx_usage_time={}

        # Time local reading segments
        tlls_percentage=float(aux_lrepl_client_time_stats[7])*100/total_computed_time
        dic_ctx_usage_time={ "key": "Time local reading segments", "value": tlls_percentage, "unit": "%" }
        list_ctx_usage_time.append(dic_ctx_usage_time)
        dic_ctx_usage_time={}

        # Time sending small files
        tsmf_percentage=float(aux_lrepl_client_time_stats[8])*100/total_computed_time
        dic_ctx_usage_time={ "key": "Time sending small files", "value": tsmf_percentage, "unit": "%" }
        list_ctx_usage_time.append(dic_ctx_usage_time)
        dic_ctx_usage_time={}

        # Time sending sketches
        tse_percentage=float(aux_lrepl_client_time_stats[9])*100/total_computed_time
        dic_ctx_usage_time={ "key": "Time sending sketches", "value": tse_percentage, "unit": "%" }
        list_ctx_usage_time.append(dic_ctx_usage_time)
        dic_ctx_usage_time={}

        # Time receiving bases
        trb_percentage=float(aux_lrepl_client_time_stats[10])*100/total_computed_time
        dic_ctx_usage_time={ "key": "Time receiving bases", "value": tse_percentage, "unit": "%" }
        list_ctx_usage_time.append(dic_ctx_usage_time)
        dic_ctx_usage_time={}

        # Time reading bases
        treadb_percentage=float(aux_lrepl_client_time_stats[11])*100/total_computed_time
        dic_ctx_usage_time={ "key": "Time reading bases", "value": treadb_percentage, "unit": "%" }
        list_ctx_usage_time.append(dic_ctx_usage_time)
        dic_ctx_usage_time={}

        # Time getting chunk info
        tgci_percentage=float(aux_lrepl_client_time_stats[12])*100/total_computed_time
        dic_ctx_usage_time={ "key": "Time getting chunk info", "value": tgci_percentage, "unit": "%" }
        list_ctx_usage_time.append(dic_ctx_usage_time)
        dic_ctx_usage_time={}

        # Time unpacking chunks of info
        tupi_percentage=float(aux_lrepl_client_time_stats[13])*100/total_computed_time
        dic_ctx_usage_time={ "key": "Time unpacking chunks of info", "value": tupi_percentage, "unit": "%" }
        list_ctx_usage_time.append(dic_ctx_usage_time)
        dic_ctx_usage_time={}

        print("que contiene list_ctx_usage_time:{}".format(list_ctx_usage_time))
        dic_auxiliar.update(dic_ctx_usage_time)
        dic_auxiliar['ctxUsageTime']=list_ctx_usage_time
        resultado.append(dic_auxiliar)
    #print("genial:{}".format(genial))
    print("resultado mio:{}".format(resultado))

    # This is the structure we need to end up having

    result = [
        {
          'ctxDetails': {
             'ctx': 1,
             'source': {
                 'host': 'dd390gcsr01.nam.nsroot.net',
                 'mtree': '/data/col1/dd390gcsr01_crebm4900_lsu1_rep'#he quitado una coma
                 },
             'destination': {
                 'host': 'dd390gcsr02.nam.nsroot.net',
                 'mtree': '/data/col1/dd390gcsr01_crebm4900_lsu1_rep'
                }
            },
          # Save PNG generated by matplotlib in app.config['STATIC_DIR_PATH'] andgive it a UUID
          # Then set this path to /static/img-uuid-here.png
          'graphImage': 'assets/replicationgraph.png',
          'ctxUsageTime': [
            { "key": "Total time spent by the replication context", "value": "11362471", "unit": "seconds" },
            { "key": "Time sending references", "value": "0.2", "unit": "%" },
            { "key": "Time sending segments", "value": "1.5", "unit": "%" },
            { "key": "Time receiving references", "value": "0.4", "unit": "%" },
            { "key": "Time waiting for references from destination", "value": "1.4", "unit": "%" },
            { "key": "Time waiting getting references", "value": "2.9", "unit": "%" },
            { "key": "Time local reading segments", "value": "93.6", "unit": "%" },
            { "key": "Time sending small files", "value": "0.0", "unit": "%" },
            { "key": "Time sending sketches", "value": "0.0", "unit": "%" },
            { "key": "Time receiving bases", "value": "0.0", "unit": "%" },
            { "key": "Time reading bases", "value": "0.0", "unit": "%" },
            { "key": "Time getting chunk info", "value": "0.0", "unit": "%" },
            { "key": "Time unpacking chunks of info", "value": "0.0", "unit": "%" }
          ]
        }


    ]


    print("result bueno:{}".format(result))
    _log.info("Analyzed %d replication contexts", len(result))

    return (jsonify(resultad),
            200,
            {'ContentType': 'application/json'})
